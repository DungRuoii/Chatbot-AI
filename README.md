# ğŸ“œ RAG System for Historical & Tourism Q&A

**Automated Q&A System with RAG (Retrieval-Augmented Generation) for Vietnamese History & Tourism**

A production-ready AI system that combines real-time web search, vector database retrieval, and large language models to provide intelligent answers about Vietnamese history and tourism.

---

## ğŸ¯ Features

âœ… **Multi-language Support** - Vietnamese, English, Chinese, Japanese, Korean, Thai  
âœ… **Role-based Responses** - Traveler, Student, Researcher, Enthusiast personas  
âœ… **Automatic Map Integration** - Extract and display coordinates on interactive maps  
âœ… **Conversation Memory** - Per-user conversation history for context-aware responses  
âœ… **Hybrid Search** - Vector search + Keyword search (Reciprocal Rank Fusion)  
âœ… **Web Fallback** - Automatic external search when local data is insufficient  
âœ… **Daily Auto-crawler** - Scheduled background crawler for knowledge base expansion  
âœ… **Production Ready** - Docker support, comprehensive logging, error handling  

---

## ğŸ“‹ Tech Stack

| Layer | Technology |
|-------|-----------|
| **Frontend** | React + TypeScript + Leaflet.js |
| **Backend** | FastAPI + Python 3.11 |
| **Vector DB** | Qdrant (semantic search) |
| **LLM Provider** | Perplexity AI / OpenAI |
| **Embedding** | Sentence-Transformers |
| **Web Scraping** | Wikipedia API + RSS Feeds |
| **Task Scheduler** | APScheduler |
| **Deployment** | Docker + Docker Compose |

---

## ğŸš€ Quick Start

### **1. Clone & Setup**
```bash

git clone <repo-url>
cd chatbot-history
python -m venv venv
```

Windows 
```
venv\Scripts\activate
```

macOS/Linux
```
source venv/bin/activate
```

### **2. Install Dependencies**
```bash

pip install -r requirements.txt
```


### **2. Environment Configuration**

Create `.env` file:
```env
 Qdrant
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=your-api-key

LLM Provider
PERPLEXITY_API_KEY=your-perplexity-key
OPENAI_API_KEY=your-openai-key

App Config
APP_NAME=RAG History Tourism
COLLECTION_NAME=history_tourism
VECTOR_SIZE=384
MAX_CONTEXT_DOCS=5
MAX_CONVERSATION_HISTORY=5
```


### **3. Run Qdrant (Docker)**
```bash

cd chatbot-history/docker
docker-compose up -d qdrant
```


### **4. Start Backend**

```bash

python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

or

python run.py
```


Backend available at: `http://localhost:8000`

API Docs: `http://localhost:8000/docs`



### **5. Ingest Data**
```bash

Full ingestion (all seed + auto-discover)
python scripts/ingest_crawler.py

Quick test (3 seed + 5 auto-discover)
python scripts/ingest_crawler.py --seed-limit 3 --auto-discover-limit 5

Only seed articles
python scripts/ingest_crawler.py --only-seed
```


### **6. Start Frontend**

```bash

cd frontend
npm install
npm start
```


Frontend available at: `http://localhost:3000`

---

## ğŸ“š Project Structure

```

chatbot-history/
â”‚
â”œâ”€â”€ app/ # Main application
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ main.py # FastAPI app + scheduler initialization
â”‚ â”œâ”€â”€ config/
â”‚ â”‚ â””â”€â”€ settings.py # Environment & app settings
â”‚ â”œâ”€â”€ db/
â”‚ â”‚ â”œâ”€â”€ qdrant_client.py # Qdrant operations (search, ingest, save)
â”‚ â”‚ â””â”€â”€ conversation_store.py # In-memory conversation storage
â”‚ â”œâ”€â”€ routes/
â”‚ â”‚ â”œâ”€â”€ ask.py # Main Q&A endpoint
â”‚ â”‚ â””â”€â”€ health.py # Health check endpoint
â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â”œâ”€â”€ factory.py # Get embedder/generator instances
â”‚ â”‚ â””â”€â”€ external_search.py # External web search fallback
â”‚ â”œâ”€â”€ data_ingestion/ # Crawler pipeline
â”‚ â”‚ â”œâ”€â”€ enhanced_pipeline.py # Main ingestion pipeline
â”‚ â”‚ â”œâ”€â”€ auto_crawler.py # Auto-discovery from categories
â”‚ â”‚ â”œâ”€â”€ wikipedia_scraper.py # Wikipedia data collection
â”‚ â”‚ â”œâ”€â”€ web_scraper.py # RSS feed scraping
â”‚ â”‚ â”œâ”€â”€ data_processor.py # Text cleaning + chunking + embedding
â”‚ â”‚ â”œâ”€â”€ deduplication.py # Duplicate detection
â”‚ â”‚ â””â”€â”€ sources/
â”‚ â”‚ â”œâ”€â”€ historical_sources.py
â”‚ â”‚ â””â”€â”€ tourism_sources.py
â”‚ â”œâ”€â”€ scheduler/
â”‚ â”‚ â””â”€â”€ crawler_scheduler.py # Background scheduler (daily crawl)
â”‚ â””â”€â”€ utils/
â”‚ â”œâ”€â”€ context.py # Context building + language detection
â”‚ â”œâ”€â”€ role_prompts.py # Role-specific prompts
â”‚ â””â”€â”€ conversation.py # Query rewriting, conversation history
â”‚
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ ingest_crawler.py # Manual ingestion script with parameters
â”‚
â”œâ”€â”€ frontend/ # React application
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ components/
â”‚ â”‚ â”‚ â”œâ”€â”€ MapView.jsx # Leaflet map component
â”‚ â”‚ â”‚ â”œâ”€â”€ ChatInput.jsx
â”‚ â”‚ â”‚ â”œâ”€â”€ ConversationHistory.jsx
â”‚ â”‚ â”‚ â”œâ”€â”€ Message.jsx
â”‚ â”‚ â”‚ â”œâ”€â”€ RoleSelect.jsx
â”‚ â”‚ â”‚ â””â”€â”€ ModelSelect.jsx
â”‚ â”‚ â”œâ”€â”€ pages/
â”‚ â”‚ â”‚ â””â”€â”€ ChatbotPage.jsx # Main chat interface
â”‚ â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â”‚ â”œâ”€â”€askService.js # API calls
â”‚ â”‚ â”‚ â”œâ”€â”€ conversationService.js
â”‚ â”‚ â”‚ â””â”€â”€ index.js
â”‚ â”‚ â”œâ”€â”€ utils/  # Utility functions
â”‚ â”‚ â”‚   â”œâ”€â”€ language.js
â”‚ â”‚ â”‚   â””â”€â”€ uuid.js
â”‚ â”‚ â”œâ”€â”€ App.css          # Global styles
â”‚ â”‚ â”œâ”€â”€ App.jsx          # Main React app component
â”‚ â”‚ â”œâ”€â”€ main.jsx         # Entry point
â”‚ â”‚ â””â”€â”€ styles.css # Styling for components
â”‚ â””â”€â”€ package.json
â”‚
â”œâ”€â”€ logs/ # Application logs
â”‚ â””â”€â”€ ingestion_*.log
â”œâ”€â”€ crawled_history.txt # Track crawled articles
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ docker-compose.yml # Docker compose setup
â”œâ”€â”€ Dockerfile # Backend container
â”œâ”€â”€ .env # Environment variables
â””â”€â”€ README.md # This file
```




---

## ğŸ”Œ API Endpoints

### **1. Ask Question**

**POST** `/ask/`

Request:
```json
{
    "prompt": "ChÃ¹a Má»™t Cá»™t á»Ÿ Ä‘Ã¢u?",
    "model": "openai",
    "deepResearch": false,
    "user_id": "user123",
    "tenancy": "traveler",
    "top_k": 5,
    "use_keyword": true
}
```


Response:
```json
{
    "prompt": "ChÃ¹a Má»™t Cá»™t á»Ÿ Ä‘Ã¢u?",
    "answer": "## ChÃ¹a Má»™t Cá»™t\n\nVá»‹ trÃ­: HÃ  Ná»™i...",
    "sources": [
  {
    "title": "ChÃ¹a Má»™t Cá»™t",
    "url": "https://vi.wikipedia.org/...",
    "score": 0.92,
    "answer_snippet": "..."
  }
  ],
    "conversation_id": "user123-memory-session",
    "mode": "rag",
    "language": "vi"
}
```


### **2. Health Check**

**GET** `/health/`

Response:
```json
{
    "status": "healthy",
    "database": {
        "status": "connected",
        "documents": 1250,
        "collection": "history_tourism"
    },
    "scheduler": {
        "status": "active",
        "jobs": [
            {
                "id": "daily_crawl",
                "name": "Daily Crawler Job",
                "trigger": "cron[hour='2', minute='0']",
                "next_run": "2025-11-02T02:00:00"
            }
        ]
    }
}
```


### **3. Get Scheduled Jobs**

**GET** `/api/scheduler/jobs`

### **4. Trigger Crawl Manually**

**POST** `/api/scheduler/run-crawl`

---

## ğŸ”„ Crawler Usage

### **Basic Commands**

```bash

python manual_expand.py --limit 50 --max-per-category 10
```

### **Parameters**

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `--seed-limit` | int | None | Limit seed articles (None = ALL) |
| `--auto-discover-limit` | int | None | Limit auto-discover articles |
| `--max-per-category` | int | 50 | Max articles per category |
| `--only-seed` | flag | False | Skip auto-discovery |



```mermaid
graph TD
    subgraph "BÆ°á»›c 1: Khá»Ÿi cháº¡y (manual_expand.py)"
        A[/"Cháº¡y lá»‡nh:<br>docker compose run --rm crawler<br>python manual_expand.py --fetch-rss"/] --> B("<b>manual_expand.py</b>: main() Ä‘Æ°á»£c gá»i");
        B --> C{"Parse cÃ¡c tham sá»‘ dÃ²ng lá»‡nh<br>--fetch-rss=True<br>--max-per-category=50<br>..."};
        C --> D["Khá»Ÿi táº¡o <b>EnhancedIngestionPipeline</b>"];
    end

    subgraph "BÆ°á»›c 2: Khá»Ÿi táº¡o Pipeline (enhanced_pipeline.py)"
        D --> E["- Khá»Ÿi táº¡o <b>WikipediaScraper</b> & <b>WebScraper</b>"];
        E --> F["- Khá»Ÿi táº¡o <b>DataProcessor</b>"];
        F --> G["- DataProcessor táº£i mÃ´ hÃ¬nh embedding<br>(all-MiniLM-L6-v2)"];
        G --> H["- DataProcessor khá»Ÿi táº¡o <b>DeduplicationManager</b>"];
        H --> I("<b>deduplication.py:</b><br>Táº£i toÃ n bá»™ 'title' vÃ  'content_hash'<br>Ä‘Ã£ cÃ³ tá»« Qdrant vÃ o bá»™ nhá»› Ä‘á»‡m");
        I --> J["- Káº¿t ná»‘i tá»›i Qdrant Client"];
    end

    J --> K{"<b>manual_expand.py:</b><br>Kiá»ƒm tra cÃ¡c cá» (flags) Ä‘Ã£ báº­t"};

    subgraph "NhÃ¡nh 1: Historical Ingestion (Náº¿u Ä‘Æ°á»£c kÃ­ch hoáº¡t)"
        K -- "run_historical = True" --> K_Historical["Báº¯t Ä‘áº§u <b>Phase 1:</b><br>ingest_phase_1_historical_foundation()"];
        
        K_Historical -- "1. Seed Articles" --> K_Seed("Äá»c SEED_HISTORICAL_ARTICLES<br>tá»« <b>historical_sources.py</b>");
        K_Seed --> K_Process_Seed("Gá»i ingest_wikipedia() Ä‘á»ƒ xá»­ lÃ½<br>danh sÃ¡ch bÃ i viáº¿t seed");
        
        K_Process_Seed -- "2. Auto-Discovery" --> K_AutoCheck{"Kiá»ƒm tra auto_discover == True"};
        K_AutoCheck -- "ÄÃºng" --> K_AutoCrawler("Khá»Ÿi táº¡o <b>AutoCrawler</b>");
        K_AutoCrawler --> K_ReadHistory("Äá»c 'crawled_history.txt'<br>Ä‘á»ƒ lá»c cÃ¡c bÃ i Ä‘Ã£ crawl");
        K_ReadHistory --> K_ReadCats("Äá»c HISTORICAL_SOURCES<br>tá»« <b>historical_sources.py</b>");
        K_ReadCats --> K_GetMembers("Láº·p qua cÃ¡c category, gá»i<br><b>wikipedia_scraper.py</b>.get_category_members()");
        K_GetMembers --> K_FilterNew("Lá»c ra cÃ¡c bÃ i viáº¿t má»›i chÆ°a Ä‘Æ°á»£c crawl");
        K_FilterNew --> K_Process_New("Gá»i ingest_wikipedia() Ä‘á»ƒ xá»­ lÃ½<br>danh sÃ¡ch bÃ i viáº¿t má»›i");
        K_Process_New --> K_MarkCrawled("ÄÃ¡nh dáº¥u cÃ¡c bÃ i má»›i vÃ o<br>'crawled_history.txt'");
        K_AutoCheck -- "Sai" --> K_MarkCrawled;
    end
    
    subgraph "NhÃ¡nh 2: RSS Feed Ingestion (Náº¿u --fetch-rss Ä‘Æ°á»£c báº­t)"
        K -- "--fetch-rss = True" --> Rss_Phase["Báº¯t Ä‘áº§u <b>Phase 2:</b><br>ingest_rss_feeds()"];
        Rss_Phase --> Rss_ReadSource("Äá»c DYNAMIC_SOURCES<br>tá»« <b>tourism_sources.py</b>");
        Rss_ReadSource --> Rss_Loop["Láº·p qua tá»«ng URL trong 'rss_feeds'"];
        Rss_Loop --> Rss_Fetch("Gá»i <b>web_scraper.py</b>.fetch_rss(url)<br>Ä‘á»ƒ táº£i tin tá»©c");
        Rss_Fetch --> Rss_Process("Gá»i ingest_articles() Ä‘á»ƒ xá»­ lÃ½<br>cÃ¡c bÃ i viáº¿t vá»«a táº£i vá»");
    end
    
    subgraph "Luá»“ng Xá»­ lÃ½ Chung (ingest_articles & process_article)"
        K_Process_Seed --> Proc_Start("Báº¯t Ä‘áº§u Luá»“ng Xá»­ lÃ½ Chung");
        K_Process_New --> Proc_Start;
        Rss_Process --> Proc_Start;
        
        Proc_Start --> Proc_Loop["Láº·p qua tá»«ng bÃ i viáº¿t"];
        Proc_Loop --> Proc_Dedupe{"<b>deduplication.py:</b><br>DÃ¹ng is_duplicate() Ä‘á»ƒ kiá»ƒm tra<br>dá»±a trÃªn 'title' vÃ  'content_hash'"};
        Proc_Dedupe -- "Bá»‹ trÃ¹ng" --> Proc_Skip["Bá» qua, ghi log 'SKIP'"];
        Proc_Dedupe -- "KhÃ´ng trÃ¹ng" --> Proc_Process("<b>data_processor.py:</b><br>LÃ m sáº¡ch, chia nhá» (chunking)<br>vÃ  táº¡o vector embedding");
        Proc_Process --> Proc_UpdateDedupe("Cáº­p nháº­t DeduplicationManager<br>vá»›i 'title' vÃ  'hash' má»›i");
        Proc_UpdateDedupe --> Proc_Save("<b>enhanced_pipeline.py:</b><br>Gá»i _save_to_qdrant() Ä‘á»ƒ lÆ°u vÃ o DB");
        Proc_Skip --> Proc_EndLoop;
        Proc_Save --> Proc_EndLoop;
        Proc_EndLoop --> Proc_Loop;
    end
    
    subgraph "BÆ°á»›c Cuá»‘i: HoÃ n táº¥t & BÃ¡o cÃ¡o"
        K_MarkCrawled --> Final_Summary;
        Rss_Process -- Háº¿t bÃ i viáº¿t --> Final_Summary;
        Proc_Loop -- Háº¿t bÃ i viáº¿t --> Final_Summary["In ra báº£n tÃ³m táº¯t thá»‘ng kÃª cuá»‘i cÃ¹ng"];
        Final_Summary --> Final_End[/Káº¿t thÃºc/];
    end

```